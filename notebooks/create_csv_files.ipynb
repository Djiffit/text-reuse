{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/konstaku/.local/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3058: DtypeWarning: Columns (67) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/estc_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['control_number',\n",
       " 'language.English',\n",
       " 'language.French',\n",
       " 'language.Latin',\n",
       " 'language.German',\n",
       " 'language.Scottish Gaelic',\n",
       " 'language.Italian',\n",
       " 'language.Greek Ancient to 1453 ',\n",
       " 'language.Welsh',\n",
       " 'language.Portuguese',\n",
       " 'language.Dutch',\n",
       " 'language.Greek Modern 1453- ',\n",
       " 'language.Hebrew',\n",
       " 'language.Spanish',\n",
       " 'language.Pahlavi',\n",
       " 'language.Swedish',\n",
       " 'language.Irish',\n",
       " 'language.Manx',\n",
       " 'language.Romance Other ',\n",
       " 'language.Algonquian Other ',\n",
       " 'language.Lithuanian',\n",
       " 'language.Turkish',\n",
       " 'language.English Old ca. 450-1100 ',\n",
       " 'language.Scots',\n",
       " 'language.Arabic',\n",
       " 'language.North American Indian Other ',\n",
       " 'language.Persian',\n",
       " 'language.French Middle ca. 1300-1600 ',\n",
       " 'language.Newari',\n",
       " 'language.Armenian',\n",
       " 'language.Tamil',\n",
       " 'language.Icelandic',\n",
       " 'language.Bengali',\n",
       " 'language.Russian',\n",
       " 'language.Malayalam',\n",
       " 'language.Danish',\n",
       " 'language.English Middle 1100-1500 ',\n",
       " 'language.Coptic',\n",
       " 'language.Mongolian',\n",
       " 'language.Gujarati',\n",
       " 'language.Malay',\n",
       " 'language.Sanskrit',\n",
       " 'language.Gothic',\n",
       " 'language.Mohawk',\n",
       " 'language.Delaware',\n",
       " 'language.Iroquoian Other ',\n",
       " 'language.Palauan',\n",
       " 'language.Arawak',\n",
       " 'language.Scottish Gaelix',\n",
       " 'multilingual',\n",
       " 'language',\n",
       " 'system_control_number',\n",
       " 'author_name',\n",
       " 'author_birth',\n",
       " 'author_death',\n",
       " 'title',\n",
       " 'publication_year_from',\n",
       " 'publication_year_till',\n",
       " 'pagecount',\n",
       " 'volnumber',\n",
       " 'volcount',\n",
       " 'parts',\n",
       " 'gatherings.original',\n",
       " 'width.original',\n",
       " 'height.original',\n",
       " 'obl.original',\n",
       " 'publication_frequency_annual',\n",
       " 'publication_frequency_text',\n",
       " 'publication_interval_from',\n",
       " 'publication_interval_till',\n",
       " 'subject_topic',\n",
       " 'publication_topic',\n",
       " 'publication_geography',\n",
       " 'original_row',\n",
       " 'publisher',\n",
       " 'latitude',\n",
       " 'longitude',\n",
       " 'publication_place',\n",
       " 'country',\n",
       " 'author_pseudonyme',\n",
       " 'author',\n",
       " 'author_gender',\n",
       " 'publication_year',\n",
       " 'publication_decade',\n",
       " 'first_edition',\n",
       " 'self_published',\n",
       " 'gatherings',\n",
       " 'width',\n",
       " 'height',\n",
       " 'obl',\n",
       " 'area',\n",
       " 'pagecount.orig',\n",
       " 'singlevol',\n",
       " 'multivol',\n",
       " 'issue',\n",
       " 'paper',\n",
       " 'paper.check',\n",
       " 'document.items',\n",
       " 'id',\n",
       " 'document_type',\n",
       " 'publication_time']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = np.array([\n",
    "    'control_number', 'system_control_number', 'author_name', 'author_birth', 'author_death', 'title', 'publication_year_from',\n",
    " 'publication_year_till',\n",
    " 'volnumber',\n",
    " 'volcount',\n",
    " 'parts',\n",
    " 'publisher',\n",
    " 'publication_place',\n",
    " 'author_pseudonyme',\n",
    " 'author',\n",
    " 'publication_year',\n",
    " 'publication_decade',\n",
    " 'id',\n",
    "])\n",
    "\n",
    "df2 = df[columns]\n",
    "df2 = df2.set_index('system_control_number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_canon = pd.read_csv('../data/estc_metadata_work_id_and_canon_status.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_canon = df_canon[df_canon['is_canon']== True]\n",
    "df_canon = df_canon.set_index('system_control_number')\n",
    "df_canon = df_canon.join(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'work_id', 'is_canon', 'control_number', 'author_name',\n",
       "       'author_birth', 'author_death', 'title', 'publication_year_from',\n",
       "       'publication_year_till', 'volnumber', 'volcount', 'parts', 'publisher',\n",
       "       'publication_place', 'author_pseudonyme', 'author', 'publication_year',\n",
       "       'publication_decade', 'id'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_canon.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create texts\n",
    "df_final = df_canon[['id', 'title']]\n",
    "df_final.to_csv('../data/texts.csv')\n",
    "text_ids = set(list(df_canon['id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create actors\n",
    "df_actors = pd.read_csv('../data/unified_actors.csv')\n",
    "df_actor_details = pd.read_csv('../data/unified_actorlinks_enriched.csv')\n",
    "df_actor_details = df_actor_details.set_index('curives')\n",
    "\n",
    "actor_ids = set(df_actors['actor_id'])\n",
    "df_actors = df_actors[['actor_id', 'name_unified']]\n",
    "names_by_id = df_actor_details[['actor_id', 'actor_name_primary']].set_index('actor_id').to_dict()['actor_name_primary']\n",
    "\n",
    "def get_name(val):\n",
    "    return names_by_id_df.get(val, 'N/A')\n",
    "\n",
    "df_actors['name'] = df_actors['actor_id'].apply(get_name)\n",
    "df_actors = df_actors.set_index('actor_id')[['name']]\n",
    "df_actors.to_csv('../data/actors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_canon_join = df_canon[['id']].join(df_actor_details)[['actor_id', 'id', 'actor_role_author', 'actor_role_printer', 'actor_role_publisher', 'actor_role_bookseller', 'actor_role_corporate_author', 'actor_role_geographic_record', 'actor_role_unknown', 'actor_role_translator', 'actor_role_editor', 'actor_role_engraver', 'actor_role_other', 'pubyear', 'publoc']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress at 5000 / 112065\n",
      "Progress at 15000 / 112065\n",
      "Progress at 25000 / 112065\n",
      "Progress at 35000 / 112065\n",
      "Progress at 45000 / 112065\n",
      "Progress at 55000 / 112065\n",
      "Progress at 65000 / 112065\n",
      "Progress at 75000 / 112065\n",
      "Progress at 85000 / 112065\n",
      "Progress at 95000 / 112065\n",
      "Progress at 105000 / 112065\n"
     ]
    }
   ],
   "source": [
    "entity_connections = []\n",
    "processed_ids = set([])\n",
    "years, places = set([]), set([])\n",
    "year_connections = []\n",
    "place_connections = []\n",
    "\n",
    "roles = ['actor_role_author', 'actor_role_printer', 'actor_role_publisher', 'actor_role_bookseller', 'actor_role_corporate_author', 'actor_role_geographic_record', 'actor_role_unknown', 'actor_role_translator', 'actor_role_editor', 'actor_role_engraver', 'actor_role_other'] \n",
    "connections = ['author', 'printer', 'publisher', 'bookseller', 'corporate author', 'geographic record', 'unknown' , 'translator', 'editor', 'engraver', 'other']\n",
    "stats = ['pubyear', 'publoc']\n",
    "\n",
    "def process_connections(row):\n",
    "    for role, connection in zip(roles, connections):\n",
    "        if row[role]:\n",
    "            entity_connections.append({'text': row['id'], 'entity': row['actor_id'], 'type': connection})\n",
    "        if row['id'] not in processed_ids:\n",
    "            processed_ids.add(row['id'])\n",
    "            if row.get('publoc'):\n",
    "                places.add(row['publoc'])\n",
    "                place_connections.append({'text': row['id'], 'place': row['publoc']})\n",
    "            if row.get('pubyear'):\n",
    "                years.add(row['pubyear'])\n",
    "                year_connections.append({'text': row['id'], 'year': row['pubyear']})    \n",
    "            \n",
    "for i, (_, row) in enumerate(df_canon_join.iterrows()):\n",
    "    \n",
    "    if i % 10000 == 5000:\n",
    "        print(f'Progress at {i} / {len(df_canon_join)}')\n",
    "    process_connections(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_df = pd.DataFrame.from_dict(year_connections).set_index('text')\n",
    "place_df = pd.DataFrame.from_dict(place_connections).set_index('text')\n",
    "entity_df = pd.DataFrame.from_dict(entity_connections).set_index('text')\n",
    "\n",
    "year_df.to_csv('../data/year_connections.csv')\n",
    "place_df.to_csv('../data/place_connections.csv')\n",
    "entity_df.to_csv('../data/entity_connections.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pairs = pd.read_csv('../data/idpairs.csv')\n",
    "pair_by_fulltext_id = text_pairs.set_index('fulltext_id').to_dict()['curives_id']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "fulltext_ids = (set([str(x) for x in text_pairs['fulltext_id']]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for last 1000 cluster files: 63.817973136901855 Progress: 1000 / 28319\n",
      "Time for last 1000 cluster files: 65.22922039031982 Progress: 2000 / 28319\n",
      "Time for last 1000 cluster files: 63.60137438774109 Progress: 3000 / 28319\n",
      "Time for last 1000 cluster files: 61.56176710128784 Progress: 4000 / 28319\n",
      "Time for last 1000 cluster files: 61.720980644226074 Progress: 5000 / 28319\n",
      "Time for last 1000 cluster files: 58.04584050178528 Progress: 6000 / 28319\n",
      "Time for last 1000 cluster files: 60.80421209335327 Progress: 7000 / 28319\n",
      "Time for last 1000 cluster files: 59.57559275627136 Progress: 8000 / 28319\n",
      "Time for last 1000 cluster files: 59.610456705093384 Progress: 9000 / 28319\n",
      "Time for last 1000 cluster files: 63.264538526535034 Progress: 10000 / 28319\n",
      "Time for last 1000 cluster files: 58.29667663574219 Progress: 11000 / 28319\n",
      "Time for last 1000 cluster files: 58.213151931762695 Progress: 12000 / 28319\n",
      "Time for last 1000 cluster files: 59.76942753791809 Progress: 13000 / 28319\n",
      "Time for last 1000 cluster files: 60.38746643066406 Progress: 14000 / 28319\n",
      "Time for last 1000 cluster files: 59.1693069934845 Progress: 15000 / 28319\n",
      "Time for last 1000 cluster files: 58.25702381134033 Progress: 16000 / 28319\n",
      "Time for last 1000 cluster files: 57.2944061756134 Progress: 17000 / 28319\n",
      "Time for last 1000 cluster files: 60.79474663734436 Progress: 18000 / 28319\n",
      "Time for last 1000 cluster files: 60.6627733707428 Progress: 19000 / 28319\n",
      "Time for last 1000 cluster files: 60.27188491821289 Progress: 20000 / 28319\n",
      "Time for last 1000 cluster files: 55.534422636032104 Progress: 21000 / 28319\n",
      "Time for last 1000 cluster files: 58.819427251815796 Progress: 22000 / 28319\n",
      "Time for last 1000 cluster files: 58.539551973342896 Progress: 23000 / 28319\n",
      "Time for last 1000 cluster files: 57.418726444244385 Progress: 24000 / 28319\n",
      "Time for last 1000 cluster files: 55.85602951049805 Progress: 25000 / 28319\n",
      "Time for last 1000 cluster files: 58.24111747741699 Progress: 26000 / 28319\n",
      "Time for last 1000 cluster files: 58.38214159011841 Progress: 27000 / 28319\n",
      "Time for last 1000 cluster files: 64.55657029151917 Progress: 28000 / 28319\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "import os\n",
    "clusters = {}\n",
    "json_files = [c for c in os.listdir('../data/clusters') if not c.endswith('.gz') and 'ipynb_checkpoints' not in c]\n",
    "\n",
    "np.random.shuffle(json_files)\n",
    "start = time.time()\n",
    "for i, json_path in enumerate(json_files):\n",
    "    with open('../data/clusters/' + json_path) as f:\n",
    "        data = json.load(f)\n",
    "        for cluster, reuses in data.items():\n",
    "            reuse_arr = []\n",
    "            for entry in reuses.get('hits', []):\n",
    "                id_ref = entry['doc_id'].split('/')[-1].split('.')[0]\n",
    "                if id_ref in fulltext_ids:\n",
    "                    try:\n",
    "                        curive = pair_by_fulltext_id[int(id_ref)]\n",
    "                        if curive in text_ids:\n",
    "                            reuse_arr.append({'text_id': id_ref, 'reuse_id': entry['id'], 'curives_id': curive})\n",
    "                    except:\n",
    "                        pass\n",
    "                        \n",
    "                    \n",
    "            if len(reuse_arr) > 0:\n",
    "                clusters[cluster] = reuse_arr\n",
    "    \n",
    "    if i % 1000 == 999:\n",
    "        print(f'Time for last 1000 cluster files: {time.time() - start}' f' Progress: {i + 1} / {len(json_files)}')\n",
    "        start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for last 1_000_000 cluster files: 3.3575356006622314 Progress: 500001 / 4964834\n",
      "Time for last 1_000_000 cluster files: 61.51358246803284 Progress: 1500001 / 4964834\n",
      "Time for last 1_000_000 cluster files: 7.419744491577148 Progress: 2500001 / 4964834\n",
      "Time for last 1_000_000 cluster files: 7.536660671234131 Progress: 3500001 / 4964834\n",
      "Time for last 1_000_000 cluster files: 10.398154020309448 Progress: 4500001 / 4964834\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "year_by_id = year_df.to_dict()['year']\n",
    "\n",
    "reuse_connections = []\n",
    "\n",
    "start = time.time()\n",
    "def create_connections(cluster, cid):\n",
    "    if len(cluster) == 1:\n",
    "        return\n",
    "    \n",
    "    cluster = list(filter(lambda c: year_by_id[c['curives_id']] == year_by_id[c['curives_id']], cluster))\n",
    "    cs = sorted([{'fulltext_id': c['text_id'], 'text_id': c['curives_id'], 'year': int(year_by_id[c['curives_id']])} for c in cluster], key=lambda x: -x['year'])\n",
    "    \n",
    "    for i in range(len(cs) - 1):\n",
    "        for j in range(i + 1, len(cs)):\n",
    "            start = cs[i]\n",
    "            end = cs[j]\n",
    "            reuse_connections.append({'from': start['text_id'], 'to': end['text_id'], 'cluster': cid})\n",
    "\n",
    "for i, (k, v) in enumerate(clusters.items()):\n",
    "    create_connections(v, k)\n",
    "    if i % 1_000_000 == 500_000:\n",
    "        print(f'Time for last 1_000_000 cluster files: {time.time() - start}' f' Progress: {i + 1} / {len(clusters)}')\n",
    "        start = time.time()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_df = pd.DataFrame.from_dict(reuse_connections).set_index('cluster')\n",
    "cluster_df.to_csv('../data/cluster_connections.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "batch = 666_666\n",
    "while count < len(df):\n",
    "    df[count:count+batch].set_index('cluster').to_csv(f'../data/cluster_connections-{count // batch}.csv')\n",
    "    count += batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39586582"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reuse_connections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('../data/cluster_connections.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df[:500_000]).set_index('cluster').to_csv(f'../data/cluster_connectionstest.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "544724"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[df['from'] == df['to'] ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39586582"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
