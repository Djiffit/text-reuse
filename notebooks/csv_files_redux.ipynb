{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/konstaku/.local/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3058: DtypeWarning: Columns (67) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/estc_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = np.array([\n",
    " 'control_number', 'system_control_number', 'author_name', 'author_birth', 'author_death', 'title', 'publication_year_from',\n",
    " 'publication_year_till',\n",
    " 'volnumber',\n",
    " 'volcount',\n",
    " 'parts',\n",
    " 'publisher',\n",
    " 'publication_place',\n",
    " 'author_pseudonyme',\n",
    " 'author',\n",
    " 'publication_year',\n",
    " 'publication_decade',\n",
    " 'id',\n",
    "])\n",
    "\n",
    "df2 = df[columns]\n",
    "df2 = df2.set_index('system_control_number')\n",
    "\n",
    "df_canon = pd.read_csv('../data/estc_metadata_work_id_and_canon_status.csv')\n",
    "\n",
    "df_canon = df_canon[df_canon['is_canon'] == True]\n",
    "df_canon = df_canon.set_index('system_control_number')\n",
    "df_canon = df_canon.join(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create texts\n",
    "df_final = df_canon[['id', 'title']]\n",
    "df_final.to_csv('../data/texts.csv')\n",
    "text_ids = set(list(df_canon['id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/konstaku/.local/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3058: DtypeWarning: Columns (2,3,7,8,9,23,24,25) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# Create actors\n",
    "df_actors = pd.read_csv('../data/unified_actors.csv')\n",
    "df_actor_details = pd.read_csv('../data/unified_actorlinks_enriched.csv')\n",
    "df_actor_details = df_actor_details.set_index('curives')\n",
    "\n",
    "actor_ids = set(df_actors['actor_id'])\n",
    "df_actors = df_actors[['actor_id', 'name_unified']]\n",
    "names_by_id = df_actor_details[['actor_id', 'actor_name_primary']].set_index('actor_id').to_dict()['actor_name_primary']\n",
    "\n",
    "def get_name(val):\n",
    "    return names_by_id.get(val, 'N/A')\n",
    "\n",
    "df_actors['name'] = df_actors['actor_id'].apply(get_name)\n",
    "df_actors = df_actors.set_index('actor_id')[['name']]\n",
    "df_actors.to_csv('../data/actors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_canon_join = df_canon[['id']].join(df_actor_details)[['actor_id', 'id', 'actor_role_author', 'actor_role_printer', 'actor_role_publisher', 'actor_role_bookseller', 'actor_role_corporate_author', 'actor_role_geographic_record', 'actor_role_unknown', 'actor_role_translator', 'actor_role_editor', 'actor_role_engraver', 'actor_role_other', 'pubyear', 'publoc']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress at 5000 / 112065\n",
      "Progress at 15000 / 112065\n",
      "Progress at 25000 / 112065\n",
      "Progress at 35000 / 112065\n",
      "Progress at 45000 / 112065\n",
      "Progress at 55000 / 112065\n",
      "Progress at 65000 / 112065\n",
      "Progress at 75000 / 112065\n",
      "Progress at 85000 / 112065\n",
      "Progress at 95000 / 112065\n",
      "Progress at 105000 / 112065\n"
     ]
    }
   ],
   "source": [
    "entity_connections = []\n",
    "processed_ids = set([])\n",
    "years, places = set([]), set([])\n",
    "year_connections = []\n",
    "place_connections = []\n",
    "\n",
    "roles = ['actor_role_author', 'actor_role_printer', 'actor_role_publisher', 'actor_role_bookseller', 'actor_role_corporate_author', 'actor_role_geographic_record', 'actor_role_unknown', 'actor_role_translator', 'actor_role_editor', 'actor_role_engraver', 'actor_role_other'] \n",
    "connections = ['author', 'printer', 'publisher', 'bookseller', 'corporate author', 'geographic record', 'unknown' , 'translator', 'editor', 'engraver', 'other']\n",
    "stats = ['pubyear', 'publoc']\n",
    "\n",
    "def process_connections(row):\n",
    "    for role, connection in zip(roles, connections):\n",
    "        if row[role]:\n",
    "            entity_connections.append({'text': row['id'], 'entity': row['actor_id'], 'type': connection})\n",
    "        if row['id'] not in processed_ids:\n",
    "            processed_ids.add(row['id'])\n",
    "            if row.get('publoc'):\n",
    "                places.add(row['publoc'])\n",
    "                place_connections.append({'text': row['id'], 'place': row['publoc']})\n",
    "            if row.get('pubyear'):\n",
    "                try:\n",
    "                    years.add(row['pubyear'])\n",
    "                    year_connections.append({'text': row['id'], 'year': int(row['pubyear'])})    \n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "for i, (_, row) in enumerate(df_canon_join.iterrows()):\n",
    "    \n",
    "    if i % 10000 == 5000:\n",
    "        print(f'Progress at {i} / {len(df_canon_join)}')\n",
    "    process_connections(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_df = pd.DataFrame.from_dict(year_connections).set_index('text')\n",
    "place_df = pd.DataFrame.from_dict(place_connections).set_index('text')\n",
    "entity_df = pd.DataFrame.from_dict(entity_connections).set_index('text')\n",
    "\n",
    "year_df.to_csv('../data/year_connections.csv')\n",
    "place_df.to_csv('../data/place_connections.csv')\n",
    "entity_df.to_csv('../data/entity_connections.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pairs = pd.read_csv('../data/idpairs.csv')\n",
    "pair_by_fulltext_id = text_pairs.set_index('fulltext_id').to_dict()['curives_id']\n",
    "fulltext_ids = (set([str(x) for x in text_pairs['fulltext_id']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for last 1000 cluster files: 62.10176467895508 Progress: 1000 / 28319\n",
      "Time for last 1000 cluster files: 66.8294587135315 Progress: 2000 / 28319\n",
      "Time for last 1000 cluster files: 68.3068585395813 Progress: 3000 / 28319\n",
      "Time for last 1000 cluster files: 67.07979726791382 Progress: 4000 / 28319\n",
      "Time for last 1000 cluster files: 68.19957613945007 Progress: 5000 / 28319\n",
      "Time for last 1000 cluster files: 67.92010760307312 Progress: 6000 / 28319\n",
      "Time for last 1000 cluster files: 66.44391870498657 Progress: 7000 / 28319\n",
      "Time for last 1000 cluster files: 66.11498737335205 Progress: 8000 / 28319\n",
      "Time for last 1000 cluster files: 64.97461605072021 Progress: 9000 / 28319\n",
      "Time for last 1000 cluster files: 63.28676247596741 Progress: 10000 / 28319\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "import os\n",
    "\n",
    "json_files = [c for c in os.listdir('../data/clusters') if not c.endswith('.gz') and 'ipynb_checkpoints' not in c]\n",
    "\n",
    "# np.random.shuffle(json_files)\n",
    "def create_connections():\n",
    "    start = time.time()\n",
    "    clusters = {}\n",
    "    for i, json_path in enumerate(json_files):\n",
    "        with open('../data/clusters/' + json_path) as f:\n",
    "            data = json.load(f)\n",
    "            for cluster, reuses in data.items():\n",
    "                reuse_arr = []\n",
    "                for entry in reuses.get('hits', []):\n",
    "                    id_ref = entry['doc_id'].split('/')[-1].split('.')[0]\n",
    "                    if id_ref in fulltext_ids:\n",
    "                        try:\n",
    "                            curive = pair_by_fulltext_id[int(id_ref)]\n",
    "                            reuse_arr.append({'text_id': id_ref, 'reuse_id': entry['id'], 'curives_id': curive})\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "                if len(reuse_arr) > 1:\n",
    "                    clusters[cluster] = reuse_arr\n",
    "\n",
    "        if i % 1000 == 999:\n",
    "            print(f'Time for last 1000 cluster files: {time.time() - start}' f' Progress: {i + 1} / {len(json_files)}')\n",
    "            start = time.time()\n",
    "    return clusters\n",
    "clusters = create_connections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for last 1_000_000 cluster files: 15.4519784450531 Progress: 500001 / 10479850\n",
      "Time for last 1_000_000 cluster files: 33.522534132003784 Progress: 1500001 / 10479850\n",
      "Time for last 1_000_000 cluster files: 37.305570125579834 Progress: 2500001 / 10479850\n",
      "Time for last 1_000_000 cluster files: 32.919373512268066 Progress: 3500001 / 10479850\n",
      "Time for last 1_000_000 cluster files: 34.43808937072754 Progress: 4500001 / 10479850\n",
      "Time for last 1_000_000 cluster files: 33.277485609054565 Progress: 5500001 / 10479850\n",
      "Time for last 1_000_000 cluster files: 22.94879722595215 Progress: 6500001 / 10479850\n",
      "Time for last 1_000_000 cluster files: 21.82454514503479 Progress: 7500001 / 10479850\n",
      "Time for last 1_000_000 cluster files: 28.396111965179443 Progress: 8500001 / 10479850\n",
      "Time for last 1_000_000 cluster files: 38.824570655822754 Progress: 9500001 / 10479850\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "year_by_id = year_df.to_dict()['year']\n",
    "\n",
    "reuse_connections = {}\n",
    "\n",
    "start = time.time()\n",
    "def create_connections(cluster, cid):\n",
    "    if len(cluster) == 1:\n",
    "        return\n",
    "    \n",
    "    cluster = list(filter(lambda c: year_by_id.get(c['curives_id'], -11) == year_by_id.get(c['curives_id'], -22), cluster))\n",
    "    uniques_texts = set([c['curives_id'] for c in cluster])\n",
    "    cs = sorted([{'text_id': t_id, 'year': int(year_by_id[t_id])} for t_id in uniques_texts], key=lambda x: -x['year'])\n",
    "    \n",
    "    for i in range(len(cs) - 1):\n",
    "        for j in range(i + 1, len(cs)):\n",
    "            start = cs[i]\n",
    "            end = cs[j]\n",
    "            start_id = start['text_id']\n",
    "            end_id = end['text_id']\n",
    "            reuse_connections[(start_id, end_id)] = reuse_connections.get((start_id, end_id), 0) + 1\n",
    "            if start['year'] == end['year']:\n",
    "                reuse_connections[(end_id, start_id)] = reuse_connections.get((end_id, start_id), 0) + 1\n",
    "\n",
    "for i, (k, v) in enumerate(clusters.items()):\n",
    "    create_connections(v, k)\n",
    "    if i % 1_000_000 == 500_000:\n",
    "        print(f'Time for last 1_000_000 cluster files: {time.time() - start}' f' Progress: {i + 1} / {len(clusters)}')\n",
    "        start = time.time()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'reuse_connections' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-6129045a6565>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'from'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'to'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'count'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreuse_connections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mconn_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_arr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'reuse_connections' is not defined"
     ]
    }
   ],
   "source": [
    "data_arr = [{'source': p[0], 'reuser': p[1], 'count': num} for p, num in reuse_connections.items()]\n",
    "\n",
    "conn_df = pd.DataFrame(data_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_df.to_csv('../data/cluster_connections_reduced.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = entity_df[entity_df['type'] == 'author'].join(df_actors, on='entity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df = df_final.set_index('id').join(year_df).join(place_df).join(authors)[['title', 'year', 'place', 'name']]\n",
    "\n",
    "joined_df.to_csv('../data/text_with_details.csv', sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " 1473.0,\n",
       " 1474.0,\n",
       " 1476.0,\n",
       " 1477.0,\n",
       " 1479.0,\n",
       " 1481.0,\n",
       " 1482.0,\n",
       " 1483.0,\n",
       " 1484.0,\n",
       " 1485.0,\n",
       " 1488.0,\n",
       " 1490.0,\n",
       " 1492.0,\n",
       " 1494.0,\n",
       " 1495.0,\n",
       " 1496.0,\n",
       " 1497.0,\n",
       " 1498.0,\n",
       " 1499.0,\n",
       " 1500.0,\n",
       " 1501.0,\n",
       " 1502.0,\n",
       " 1503.0,\n",
       " 1504.0,\n",
       " 1505.0,\n",
       " 1506.0,\n",
       " 1507.0,\n",
       " 1508.0,\n",
       " 1509.0,\n",
       " 1510.0,\n",
       " 1511.0,\n",
       " 1512.0,\n",
       " 1513.0,\n",
       " 1514.0,\n",
       " 1515.0,\n",
       " 1516.0,\n",
       " 1517.0,\n",
       " 1518.0,\n",
       " 1519.0,\n",
       " 1520.0,\n",
       " 1521.0,\n",
       " 1522.0,\n",
       " 1523.0,\n",
       " 1524.0,\n",
       " 1525.0,\n",
       " 1526.0,\n",
       " 1527.0,\n",
       " 1528.0,\n",
       " 1529.0,\n",
       " 1530.0,\n",
       " 1531.0,\n",
       " 1532.0,\n",
       " 1533.0,\n",
       " 1534.0,\n",
       " 1535.0,\n",
       " 1536.0,\n",
       " 1537.0,\n",
       " 1538.0,\n",
       " 1539.0,\n",
       " 1540.0,\n",
       " 1541.0,\n",
       " 1542.0,\n",
       " 1543.0,\n",
       " 1544.0,\n",
       " 1545.0,\n",
       " 1546.0,\n",
       " 1547.0,\n",
       " 1548.0,\n",
       " 1549.0,\n",
       " 1550.0,\n",
       " 1551.0,\n",
       " 1552.0,\n",
       " 1553.0,\n",
       " 1554.0,\n",
       " 1555.0,\n",
       " 1556.0,\n",
       " 1557.0,\n",
       " 1558.0,\n",
       " 1559.0,\n",
       " 1560.0,\n",
       " 1561.0,\n",
       " 1562.0,\n",
       " 1563.0,\n",
       " 1564.0,\n",
       " 1565.0,\n",
       " 1566.0,\n",
       " 1567.0,\n",
       " 1568.0,\n",
       " 1569.0,\n",
       " 1570.0,\n",
       " 1571.0,\n",
       " 1572.0,\n",
       " 1573.0,\n",
       " 1574.0,\n",
       " 1575.0,\n",
       " 1576.0,\n",
       " 1577.0,\n",
       " 1578.0,\n",
       " 1579.0,\n",
       " 1580.0,\n",
       " 1581.0,\n",
       " 1582.0,\n",
       " 1583.0,\n",
       " 1584.0,\n",
       " 1585.0,\n",
       " 1586.0,\n",
       " 1587.0,\n",
       " 1588.0,\n",
       " 1589.0,\n",
       " 1590.0,\n",
       " 1591.0,\n",
       " 1592.0,\n",
       " 1593.0,\n",
       " 1594.0,\n",
       " 1595.0,\n",
       " 1596.0,\n",
       " 1597.0,\n",
       " 1598.0,\n",
       " 1599.0,\n",
       " 1600.0,\n",
       " 1601.0,\n",
       " 1602.0,\n",
       " 1603.0,\n",
       " 1604.0,\n",
       " 1605.0,\n",
       " 1606.0,\n",
       " 1607.0,\n",
       " 1608.0,\n",
       " 1609.0,\n",
       " 1610.0,\n",
       " 1611.0,\n",
       " 1612.0,\n",
       " 1613.0,\n",
       " 1614.0,\n",
       " 1615.0,\n",
       " 1616.0,\n",
       " 1617.0,\n",
       " 1618.0,\n",
       " 1619.0,\n",
       " 1620.0,\n",
       " 1621.0,\n",
       " 1622.0,\n",
       " 1623.0,\n",
       " 1624.0,\n",
       " 1625.0,\n",
       " 1626.0,\n",
       " 1627.0,\n",
       " 1628.0,\n",
       " 1629.0,\n",
       " 1630.0,\n",
       " 1631.0,\n",
       " 1632.0,\n",
       " 1633.0,\n",
       " 1634.0,\n",
       " 1635.0,\n",
       " 1636.0,\n",
       " 1637.0,\n",
       " 1638.0,\n",
       " 1639.0,\n",
       " 1640.0,\n",
       " 1641.0,\n",
       " 1642.0,\n",
       " 1643.0,\n",
       " 1644.0,\n",
       " 1645.0,\n",
       " 1646.0,\n",
       " 1647.0,\n",
       " 1648.0,\n",
       " 1649.0,\n",
       " 1650.0,\n",
       " 1651.0,\n",
       " 1652.0,\n",
       " 1653.0,\n",
       " 1654.0,\n",
       " 1655.0,\n",
       " 1656.0,\n",
       " 1657.0,\n",
       " 1658.0,\n",
       " 1659.0,\n",
       " 1660.0,\n",
       " 1661.0,\n",
       " 1662.0,\n",
       " 1663.0,\n",
       " 1664.0,\n",
       " 1665.0,\n",
       " 1666.0,\n",
       " 1667.0,\n",
       " 1668.0,\n",
       " 1669.0,\n",
       " 1670.0,\n",
       " 1671.0,\n",
       " 1672.0,\n",
       " 1673.0,\n",
       " 1674.0,\n",
       " 1675.0,\n",
       " 1676.0,\n",
       " 1677.0,\n",
       " 1678.0,\n",
       " 1679.0,\n",
       " 1680.0,\n",
       " 1681.0,\n",
       " 1682.0,\n",
       " 1683.0,\n",
       " 1684.0,\n",
       " 1685.0,\n",
       " 1686.0,\n",
       " 1687.0,\n",
       " 1688.0,\n",
       " 1689.0,\n",
       " 1690.0,\n",
       " 1691.0,\n",
       " 1692.0,\n",
       " 1693.0,\n",
       " 1694.0,\n",
       " 1695.0,\n",
       " 1696.0,\n",
       " 1697.0,\n",
       " 1698.0,\n",
       " 1699.0,\n",
       " 1700.0,\n",
       " 1701.0,\n",
       " 1702.0,\n",
       " 1703.0,\n",
       " 1704.0,\n",
       " 1705.0,\n",
       " 1706.0,\n",
       " 1707.0,\n",
       " 1708.0,\n",
       " 1709.0,\n",
       " 1710.0,\n",
       " 1711.0,\n",
       " 1712.0,\n",
       " 1713.0,\n",
       " 1714.0,\n",
       " 1715.0,\n",
       " 1716.0,\n",
       " 1717.0,\n",
       " 1718.0,\n",
       " 1719.0,\n",
       " 1720.0,\n",
       " 1721.0,\n",
       " 1722.0,\n",
       " 1723.0,\n",
       " 1724.0,\n",
       " 1725.0,\n",
       " 1726.0,\n",
       " 1727.0,\n",
       " 1728.0,\n",
       " 1729.0,\n",
       " 1730.0,\n",
       " 1731.0,\n",
       " 1732.0,\n",
       " 1733.0,\n",
       " 1734.0,\n",
       " 1735.0,\n",
       " 1736.0,\n",
       " 1737.0,\n",
       " 1738.0,\n",
       " 1739.0,\n",
       " 1740.0,\n",
       " 1741.0,\n",
       " 1742.0,\n",
       " 1743.0,\n",
       " 1744.0,\n",
       " 1745.0,\n",
       " 1746.0,\n",
       " 1747.0,\n",
       " 1748.0,\n",
       " 1749.0,\n",
       " 1750.0,\n",
       " 1751.0,\n",
       " 1752.0,\n",
       " 1753.0,\n",
       " 1754.0,\n",
       " 1755.0,\n",
       " 1756.0,\n",
       " 1757.0,\n",
       " 1758.0,\n",
       " 1759.0,\n",
       " 1760.0,\n",
       " 1761.0,\n",
       " 1762.0,\n",
       " 1763.0,\n",
       " 1764.0,\n",
       " 1765.0,\n",
       " 1766.0,\n",
       " 1767.0,\n",
       " 1768.0,\n",
       " 1769.0,\n",
       " 1770.0,\n",
       " 1771.0,\n",
       " 1772.0,\n",
       " 1773.0,\n",
       " 1774.0,\n",
       " 1775.0,\n",
       " 1776.0,\n",
       " 1777.0,\n",
       " 1778.0,\n",
       " 1779.0,\n",
       " 1780.0,\n",
       " 1781.0,\n",
       " 1782.0,\n",
       " 1783.0,\n",
       " 1784.0,\n",
       " 1785.0,\n",
       " 1786.0,\n",
       " 1787.0,\n",
       " 1788.0,\n",
       " 1789.0,\n",
       " 1790.0,\n",
       " 1791.0,\n",
       " 1792.0,\n",
       " 1793.0,\n",
       " 1794.0,\n",
       " 1795.0,\n",
       " 1796.0,\n",
       " 1797.0,\n",
       " 1798.0,\n",
       " 1799.0,\n",
       " 1800.0,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(joined_df['year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_df = pd.DataFrame.from_dict(year_connections).set_index('text')\n",
    "place_df = pd.DataFrame.from_dict(place_connections).set_index('text')\n",
    "entity_df = pd.DataFrame.from_dict(entity_connections).set_index('text')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
